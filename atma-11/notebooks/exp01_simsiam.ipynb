{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# overview\n",
    "- https://www.guruguru.science/competitions/17/discussions/a39d588e-aff2-4728-8323-b07f15563552/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default package\n",
    "import logging\n",
    "import sys\n",
    "import os \n",
    "import pathlib\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third party package\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision \n",
    "import numpy as np\n",
    "import lightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my package\n",
    "sys.path.append(os.path.join(pathlib.Path().resolve(),\"../\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload settings\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger\n",
    "logger=logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph settings\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:current directory: /workspaces/load_to_goal/GitHub/kaggle-past-comp/atma-11\n"
     ]
    }
   ],
   "source": [
    "def cd_project_root_()->None:\n",
    "    current=pathlib.Path().cwd()\n",
    "    if current.stem==\"notebooks\":\n",
    "        os.chdir(current.parent)\n",
    "    logger.info(f\"current directory: {pathlib.Path().cwd()}\")\n",
    "\n",
    "cd_project_root_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "num_workers = 2\n",
    "batch_size = 32\n",
    "seed = 1\n",
    "epochs = 10\n",
    "input_size = 224\n",
    "\n",
    "# dimension of the embeddings\n",
    "num_ftrs = 512\n",
    "# dimension of the output of the prediction and projection heads\n",
    "out_dim = proj_hidden_dim = 512\n",
    "# the prediction head uses a bottleneck architecture\n",
    "pred_hidden_dim = 128\n",
    "# use 2 layers in the projection head\n",
    "num_mlp_layers = 2\n",
    "\n",
    "input_dir=\"data/raw/photos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the augmentations for self-supervised learning\n",
    "collate_fn = lightly.data.ImageCollateFunction(\n",
    "    input_size=input_size,\n",
    "    # require invariance to flips and rotations\n",
    "    hf_prob=0.5,\n",
    "    vf_prob=0.5,\n",
    "    rr_prob=0.5,\n",
    "    # satellite images are all taken from the same height\n",
    "    # so we use only slight random cropping\n",
    "    min_scale=0.5,\n",
    "    # use a weak color jitter for invariance w.r.t small color changes\n",
    "    cj_prob=0.2,\n",
    "    cj_bright=0.1,\n",
    "    cj_contrast=0.1,\n",
    "    cj_hue=0.1,\n",
    "    cj_sat=0.1,\n",
    ")\n",
    "\n",
    "# create a lightly dataset for training, since the augmentations are handled\n",
    "# by the collate function, there is no need to apply additional ones here\n",
    "dataset_train_simsiam = lightly.data.LightlyDataset(\n",
    "    input_dir=input_dir\n",
    ")\n",
    "\n",
    "# create a dataloader for training\n",
    "dataloader_train_simsiam = torch.utils.data.DataLoader(\n",
    "    dataset_train_simsiam,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "# create a torchvision transformation for embedding the dataset after training\n",
    "# here, we resize the images to match the input size during training and apply\n",
    "# a normalization of the color channel based on statistics from imagenet\n",
    "test_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((input_size, input_size)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean=lightly.data.collate.imagenet_normalize['mean'],\n",
    "        std=lightly.data.collate.imagenet_normalize['std'],\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# create a lightly dataset for embedding\n",
    "dataset_test = lightly.data.LightlyDataset(\n",
    "    input_dir=input_dir,\n",
    "    transform=test_transforms\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# create a dataloader for embedding\n",
    "dataloader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/load_to_goal/GitHub/kaggle-past-comp/.venv/lib/python3.8/site-packages/lightly/models/simsiam.py:65: Warning: The high-level building block SimSiam will be deprecated in version 1.2.0. Use low-level building blocks instead. See https://docs.lightly.ai/lightly.models.html for more information\n",
      "  warnings.warn(Warning(\n"
     ]
    }
   ],
   "source": [
    "resnet = torchvision.models.resnet18(pretrained=False)\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "# create the SimSiam model using the backbone from above\n",
    "model = lightly.models.SimSiam(\n",
    "    backbone,\n",
    "    num_ftrs=num_ftrs,\n",
    "    proj_hidden_dim=pred_hidden_dim,\n",
    "    pred_hidden_dim=pred_hidden_dim,\n",
    "    out_dim=out_dim,\n",
    "    #num_mlp_layers=num_mlp_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = lightly.loss.SymNegCosineSimilarityLoss()\n",
    "\n",
    "# scale the learning rate\n",
    "lr = 0.05 * batch_size / 256\n",
    "# use SGD with momentum and weight decay\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   0] Loss = -0.84 | Collapse Level: 0.09 / 1.00\n",
      "[Epoch   1] Loss = -0.89 | Collapse Level: 0.10 / 1.00\n",
      "[Epoch   2] Loss = -0.91 | Collapse Level: 0.11 / 1.00\n",
      "[Epoch   3] Loss = -0.91 | Collapse Level: 0.09 / 1.00\n",
      "[Epoch   4] Loss = -0.90 | Collapse Level: 0.10 / 1.00\n",
      "[Epoch   5] Loss = -0.91 | Collapse Level: 0.09 / 1.00\n",
      "[Epoch   6] Loss = -0.91 | Collapse Level: 0.10 / 1.00\n",
      "[Epoch   7] Loss = -0.91 | Collapse Level: 0.08 / 1.00\n",
      "[Epoch   8] Loss = -0.91 | Collapse Level: 0.07 / 1.00\n",
      "[Epoch   9] Loss = -0.91 | Collapse Level: 0.07 / 1.00\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "avg_loss = 0.\n",
    "avg_output_std = 0.\n",
    "for e in range(epochs):\n",
    "\n",
    "    for (x0, x1), _, _ in dataloader_train_simsiam:\n",
    "\n",
    "        # move images to the gpu\n",
    "        x0 = x0.to(device)\n",
    "        x1 = x1.to(device)\n",
    "\n",
    "        # run the model on both transforms of the images\n",
    "        # the output of the simsiam model is a y containing the predictions\n",
    "        # and projections for each input x\n",
    "        y0, y1 = model(x0, x1)\n",
    "\n",
    "        # backpropagation\n",
    "        loss = criterion(y0, y1)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # calculate the per-dimension standard deviation of the outputs\n",
    "        # we can use this later to check whether the embeddings are collapsing\n",
    "        output, _ = y0\n",
    "        output = output.detach()\n",
    "        output = torch.nn.functional.normalize(output, dim=1)\n",
    "\n",
    "        output_std = torch.std(output, 0)\n",
    "        output_std = output_std.mean()\n",
    "\n",
    "        # use moving averages to track the loss and standard deviation\n",
    "        w = 0.9\n",
    "        avg_loss = w * avg_loss + (1 - w) * loss.item()\n",
    "        avg_output_std = w * avg_output_std + (1 - w) * output_std.item()\n",
    "\n",
    "    # the level of collapse is large if the standard deviation of the l2\n",
    "    # normalized output is much smaller than 1 / sqrt(dim)\n",
    "    collapse_level = max(0., 1 - math.sqrt(out_dim) * avg_output_std)\n",
    "    # print intermediate results\n",
    "    print(f'[Epoch {e:3d}] '\n",
    "        f'Loss = {avg_loss:.2f} | '\n",
    "        f'Collapse Level: {collapse_level:.2f} / 1.00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.backbone.state_dict(),\"data/processed/ssl/211113_simsiam.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 512])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.std(output, 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.1136, 0.1041, 0.0994, 0.1229, 0.1057, 0.1083, 0.1237, 0.1041, 0.1255,\n",
       "        0.1390, 0.1141, 0.1241, 0.1267, 0.1095, 0.1613, 0.1691, 0.1140, 0.1211,\n",
       "        0.1041, 0.1018, 0.1017, 0.1752, 0.1172, 0.1056, 0.0994, 0.0969, 0.1471,\n",
       "        0.1115, 0.1263, 0.1370, 0.1080, 0.0592], device='cuda:0'),\n",
       "indices=tensor([433, 480, 314, 452,  35, 375, 364, 247, 364, 108, 386, 364, 368, 386,\n",
       "        108, 224, 364, 224, 314, 483,  35, 393, 108,  35, 112, 323, 157, 314,\n",
       "        157, 157, 364, 252], device='cuda:0'))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0399, device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8644692897796631"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor=torch.randn(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9647,  0.1292, -0.2295],\n",
       "        [-0.8550,  0.4210,  0.3028]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.normalize(tensor, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5962,  0.2138, -0.3797],\n",
       "        [-1.7328,  0.8532,  0.6137]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 512])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y0[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 512])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y0[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimSiam(\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (projection_mlp): SimSiamProjectionHead(\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=128, out_features=512, bias=True)\n",
       "      (7): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (prediction_mlp): SimSiamPredictionHead(\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=128, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c2c4601ccdfd64cd6dade88f1f1d6c032ab6cc4b454503ba5b97299d8db8f0b7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit ('.venv': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
